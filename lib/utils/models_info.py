import matplotlib.pyplot as plt
import pandas as pd

models = {
    "deepscaler": {
        "parameters": 1.5,
        "size": 3.6,
        "algorithm": "reasoning"
    },
    "mistral-nemo": {
        "parameters": 12.0,
        "size": 7.1,
        "algorithm": "reasoning"
    },
    "mathstral": {
        "parameters": 7.0,
        "size": 4.1,
        "algorithm": "math"
    },
    "qwen2.5:14b": {
        "parameters": 14.0,
        "size": 9.0,
        "algorithm": "text"
    },
    "qwen2.5:7b": {
        "parameters": 7.0,
        "size": 4.7,
        "algorithm": "text"
    },
    "qwen2.5:1.5b": {
        "parameters": 1.5,
        "size": 0.986,
        "algorithm": "text"
    },
    "qwen2-math:7b": {
        "parameters": 7.0,
        "size": 4.4,
        "algorithm": "math"
    },
    "qwen2-math:1.5b": {
        "parameters": 1.5,
        "size": 0.934,
        "algorithm": "math"
    },
    "openthinker": {
        "parameters": 7.0,
        "size": 4.7,
        "algorithm": "reasoning"
    },
    "smallthinker": {
        "parameters": 3.0,
        "size": 3.6,
        "algorithm": "reasoning"
    },
    "phi4": {
        "parameters": 14.0,
        "size": 9.1,
        "algorithm": "text"
    },
    "phi3.5": {
        "parameters": 3.8,
        "size": 2.2,
        "algorithm": "text"
    },
    "gemma2": {
        "parameters": 9.0,
        "size": 5.4,
        "algorithm": "text"
    },
    "gemma2:2b": {
        "parameters": 2.0,
        "size": 1.6,
        "algorithm": "text"
    },
    "gemma2:27b": {
        "parameters": 27.0,
        "size": 16.0,
        "algorithm": "text"
    },
    "llava": {
        "parameters": 7.0,
        "size": 4.7,
        "algorithm": "vision"
    },
    "deepseek-r1": {
        "parameters": 7.0,
        "size": 7.0,
        "algorithm": "reasoning"
    },
    "llama3.2": {
        "parameters": 3.0,
        "size": 2.0,
        "algorithm": "text"
    },
    "llama3.2-vision": {
        "parameters": 11.0,
        "size": 7.9,
        "algorithm": "vision"
    },
    "mistral": {
        "parameters": 7.0,
        "size": 4.1,
        "algorithm": "text"
    },
    "mistral-small": {
        "parameters": 24.0,
        "size": 14.0,
        "algorithm": "text"
    },
    "llava-llama3": {
        "parameters": 8.0,
        "size": 5.5,
        "algorithm": "vision"
    },
    "minicpm-v": {
        "parameters": 8.0,
        "size": 5.5,
        "algorithm": "vision",
    },
    "moondream": {
        "parameters": 1.8,
        "size": 1.7,
        "algorithm": "vision"
    },
    "llava-phi3": {
        "parameters": 3.8,
        "size": 2.9,
        "algorithm": "vision"
    }
}

models_data = {
    "Modelo": [
        "deepseek-r1", "deepscaler", "gemma2", "llava", "llava-llama3", "llava-phi3",
        "llama3.2", "llama3.2-vision", "mathstral", "minicpm-v", "mistral", "mistral-nemo",
        "mistral-small", "moondream", "openthinker", "phi3.5", "phi4", "qwen2-math", "qwen2.5", "smallthinker"
    ],
    "Imagem": [
        "https://images.fastcompany.com/image/upload/f_webp,q_auto,c_fit,w_1024,h_1024/wp-cms-2/2025/01/i-0-91268357-deepseek-logo.jpg",
        "https://avatars.githubusercontent.com/u/174067447?s=200&v=4",
        "https://ollama.com/assets/library/gemma2/58a4be20-b402-4dfa-8f1d-05d820f1204f",
        "https://registry.npmmirror.com/@lobehub/icons-static-png/1.24.0/files/dark/llava-color.png",
        "https://ollama.com/assets/library/llava-llama3/dc3b65cd-62de-45cd-93f9-5c6da62214fa",
        "https://ollama.com/assets/library/llava-llama3/dc3b65cd-62de-45cd-93f9-5c6da62214fa",
        "https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/69129b55-6798-43cd-92b5-0203f5d5a2f3/10.png?t=1730375002",
        "https://media.beehiiv.com/cdn-cgi/image/fit=scale-down,format=auto,onerror=redirect,quality=80/uploads/asset/file/69129b55-6798-43cd-92b5-0203f5d5a2f3/10.png?t=1730375002",
        "https://ollama.com/assets/library/mathstral/d21307b1-fe6d-4ca6-ab07-f2482a75cdca",
        "https://ollama.com/assets/library/minicpm-v/9252c73d-2c9c-434c-8a34-21f4d5cdd25e",
        "https://github.com/jmorganca/ollama/assets/3325447/d6be0694-eb35-417b-8f08-47d3b6c2a171",
        "https://github.com/jmorganca/ollama/assets/3325447/d6be0694-eb35-417b-8f08-47d3b6c2a171",
        "https://github.com/jmorganca/ollama/assets/3325447/d6be0694-eb35-417b-8f08-47d3b6c2a171",
        "https://replicate.delivery/pbxt/KZKNhDQHqycw8Op7w056J8YTX5Bnb7xVcLiyB4le7oUgT2cY/moondream2.png",
        "https://ollama.com/assets/library/openthinker/0ef3a0d3-aae1-4855-b56b-50875e9683e8",
        "https://media.licdn.com/dms/image/v2/D4D12AQE-07X1ijhiFg/article-cover_image-shrink_600_2000/article-cover_image-shrink_600_2000/0/1716483690066?e=2147483647&v=beta&t=9BUj5e8xKzVvHQcK2LHsCwwl7F5EPLxM_DU8bxff-kE",
        "https://www.aisharenet.com/wp-content/uploads/2025/01/bbde9e53c7505db.jpg",
        "https://ollama.com/assets/library/qwen2-math/c0159f63-bf4d-4b0d-9b92-ea44c48d34c1",
        "https://ollama.com/assets/library/qwen2.5/4b4f719f-c327-489e-8dc1-89a455c21e89",
        "https://ollama.com/assets/library/smallthinker/1d25cb29-e27d-492c-be53-ce79b20def5b"
    ],
    "Par√¢metros": [
        [7.0], [1.5], [2.0, 9.0, 27], [7.0], [8.0], [3.8], [3.0], [11.0], [7.0], [8.0],
        [7.0], [12.0], [24.0], [1.8], [7.0], [3.8], [14.0], [7.0, 1.5], [14.0, 7.0, 1.5], [3.0]
    ],
    "Tamanho (Em GB)": [
        [7.0], [3.6], [1.6, 5.4, 16.0], [4.7], [5.5], [2.9], [2.0], [7.9], [4.1], [5.5],
        [4.1], [7.1], [14.0], [1.7], [4.7], [2.2], [9.1], [4.4, 0.934], [9.0, 4.7, 0.986], [3.6]
    ],
    "Algoritmo": [
        "reasoning", "reasoning", "text", "vision", "vision", "vision", "text", "vision",
        "math", "vision", "text", "reasoning", "text", "vision", "reasoning", "reasoning",
        "text", "math", "text", "reasoning"
    ],
    "Descri√ß√£o": [
        "Primeira gera√ß√£o de modelos de racioc√≠nio do DeepSeek com desempenho compar√°vel ao OpenAI-o1, incluindo seis modelos densos extra√≠dos do DeepSeek-R1 com base no Llama e no Qwen.",
        "Uma vers√£o de fine-tunning do Deepseek-R1-Distilled-Qwen-1.5B que supera o desempenho do o1-preview da OpenAI com apenas 1.5B par√¢metros em avalia√ß√µes matem√°ticas populares.",
        "O Google Gemma 2 √© um modelo eficiente e de alto desempenho, projetado pela Google, com uma arquitetura totalmente nova, projetada para desempenho e efici√™ncia l√≠deres de mercado.",
        "üåã LLaVA √© um novo modelo multimodal grande treinado de ponta a ponta que combina um codificador de vis√£o e Vicuna para compreens√£o visual e de linguagem de uso geral. Atualizado para a vers√£o 1.6.",
        "Um modelo LLaVA aprimorado do Llama 3 Instruct com melhores pontua√ß√µes em v√°rios benchmarks.",
        "llava-phi3 √© um modelo LLaVA aprimorado a partir do Phi 3 Mini 4k, com fortes benchmarks de desempenho no mesmo n√≠vel do modelo LLaVA original",
        "A cole√ß√£o Meta Llama 3.2 de modelos multil√≠ngues de grandes linguagens (LLMs) √© uma cole√ß√£o de generativos pr√©-treinados e ajustados por instru√ß√µes. Os modelos somente texto ajustados por instru√ß√µes Llama 3.2 s√£o otimizados para casos de uso de di√°logo multil√≠ngue, incluindo tarefas de recupera√ß√£o e sumariza√ß√£o de agentes.",
        "Llama 3.2 Vision √© uma cole√ß√£o de modelos generativos de racioc√≠nio de imagem ajustados por instru√ß√µes em tamanhos 11B e 90B. Eles s√£o otimizados para reconhecimento visual, racioc√≠nio de imagem, legendas e respostas a perguntas gerais sobre uma imagem.",
        "Mistral AI est√° contribuindo com o Mathstral para a comunidade cient√≠fica para refor√ßar os esfor√ßos em problemas matem√°ticos avan√ßados que exigem racioc√≠nio l√≥gico complexo e multietapas. O lan√ßamento do Mathstral √© parte de seu esfor√ßo mais amplo para dar suporte a projetos acad√™micos ‚Äî foi produzido no contexto da colabora√ß√£o da Mistral AI com o Projeto Numina.",
        "O MiniCPM-V 2.6 √© o modelo mais recente e capaz da s√©rie MiniCPM-V. Ele apresenta novos recursos para compreens√£o de v√°rias imagens e v√≠deos. Os recursos not√°veis do MiniCPM-V 2.6 incluem: Desempenho l√≠der, compreens√£o de v√°rias imagens e aprendizado em contexto, forte capacidade de OCR e efici√™ncia superior, al√©m de seu tamanho amig√°vel.",
        "Mistral √© um modelo de par√¢metro 7B, distribu√≠do com a licen√ßa Apache. Ele est√° dispon√≠vel tanto em instruct (seguimento de instru√ß√µes) quanto em text completion.",
        "Mistral NeMo √© um modelo 12B constru√≠do em colabora√ß√£o com a NVIDIA. Mistral NeMo oferece uma grande janela de contexto de at√© 128k tokens. Seu racioc√≠nio, conhecimento de mundo e precis√£o de codifica√ß√£o s√£o de √∫ltima gera√ß√£o em sua categoria de tamanho. Como depende de arquitetura padr√£o, Mistral NeMo √© f√°cil de usar e um substituto imediato em qualquer sistema que use Mistral 7B.",
        "O Mistral Small 3 estabelece um novo padr√£o na categoria de modelos de linguagem grandes ‚Äúpequenos‚Äù abaixo de 70B, ostentando par√¢metros de 24B e alcan√ßando recursos de √∫ltima gera√ß√£o compar√°veis a modelos maiores.",
        "üåî moondream2 √© um pequeno modelo de linguagem de vis√£o projetado para rodar eficientemente em dispositivos menores.",
        "Uma fam√≠lia de modelos de racioc√≠nio totalmente de c√≥digo aberto constru√≠da usando um conjunto de dados a partir de fine-tunning do Qwen2.5, superando o DeepSeek-R1 em alguns benchmarks",
        "Phi-3.5-mini √© um modelo aberto leve e de √∫ltima gera√ß√£o, criado com base em conjuntos de dados usados ‚Äã‚Äãpara Phi-3 ‚Äî dados sint√©ticos e sites filtrados dispon√≠veis publicamente, com foco em dados de alt√≠ssima qualidade e densos em racioc√≠nio.",
        "Phi-4 √© um modelo aberto de √∫ltima gera√ß√£o, com 14B par√¢metros, constru√≠do sobre uma mistura de conjuntos de dados sint√©ticos, dados de sites de dom√≠nio p√∫blico filtrados e livros acad√™micos adquiridos e conjuntos de dados de perguntas e respostas.",
        "Qwen2 Math √© uma s√©rie de modelos de linguagem matem√°tica especializados desenvolvidos com base nos Qwen2 LLMs, que superam significativamente as capacidades matem√°ticas de modelos de c√≥digo aberto e at√© mesmo modelos de c√≥digo fechado (por exemplo, GPT4o).",
        "Os modelos Qwen2.5 s√£o pr√©-treinados no mais recente conjunto de dados de larga escala do Alibaba, abrangendo at√© 18 trilh√µes de tokens. O modelo suporta at√© 128K tokens e tem suporte multil√≠ngue.",
        "Um novo modelo de racioc√≠nio pequeno, ajustado a partir do modelo Qwen 2.5 3B Instruct.",
    ],
}


def plot_parameters_x_size():
    # Dicion√°rio de cores para cada algoritmo
    colors = {
        "reasoning": "red",
        "math": "blue",
        "text": "green",
        "vision": "orange"
    }

    # Preparar os dados para o plot a partir do dicion√°rio global 'models'
    parameters = [models[model]["parameters"] for model in models]
    sizes = [models[model]["size"] for model in models]
    algorithms = [models[model]["algorithm"] for model in models]

    # Cria a figura e o eixo
    fig, ax = plt.subplots(figsize=(12, 8))

    # Para cada algoritmo distinto, plota os pontos correspondentes
    for alg in set(algorithms):
        indices = [i for i, a in enumerate(algorithms) if a == alg]
        ax.scatter(
            [parameters[i] for i in indices],
            [sizes[i] for i in indices],
            label=alg,
            color=colors[alg]
        )

    ax.set_xlabel('Par√¢metros (Bilh√µes)')
    ax.set_ylabel('Tamanho (GB)')
    ax.set_title('Par√¢metros vs. Tamanho dos Modelos')
    ax.legend()
    plt.tight_layout()

    return fig
