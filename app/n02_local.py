import streamlit as st
import lib.utils as utils
from lib.utils import plots
import pandas as pd
import random
from lib.utils.models_info import models_data
random.seed(42)
from itertools import product
import uuid

def show_table_metrics(models, questions, table):
    table_type = [
        "Desempenho de Acerto x Erros", "M√©tricas de Tempo por Modelo", "Tempo Total por Modelo",
        "Correla√ß√£o Tamanho x Acur√°cia", "Correla√ß√£o Tempo M√©dio x Acur√°cia", "Correla√ß√£o Tempo x Tamanho",
        "Desempenho por Disciplina (Agrupado por Modelo)", "Desempenho por Disciplina (Agrupado por Disciplina)",
        "Tempo por Disciplina (Agrupado por Modelo)", "Tempo por Disciplina (Agrupado por Disciplina)",
    ]
    
    plot_type = st.selectbox("Seelcione uma forma de visualiza√ß√£o: ", table_type, index=0, key='unic' + '-'.join(models))
    
    match(plot_type):
        case "Desempenho de Acerto x Erros":
            return plots.model_performance(table)
        case "M√©tricas de Tempo por Modelo":
            return plots.time_metrics(table)
        case "Tempo Total por Modelo":
            return plots.time_metrics_total(table)
        case "Correla√ß√£o Tamanho x Acur√°cia":
            return plots.correlation(table, x="Size", y="Acc")
        case "Correla√ß√£o Tempo M√©dio x Acur√°cia":
            return plots.correlation(table, x="Tavg", y="Acc")
        case "Correla√ß√£o Tempo x Tamanho":
            return plots.correlation(table, x="Tavg", y="Size")
        case "Desempenho por Disciplina (Agrupado por Modelo)":
            return plots.discipline_performance(models, questions,True, False)
        case "Desempenho por Disciplina (Agrupado por Disciplina)":
            return plots.discipline_performance(models, questions, False, False)
        case "Tempo por Disciplina (Agrupado por Modelo)":
            return plots.discipline_time_performance(models, questions, True)
        case "Tempo por Disciplina (Agrupado por Disciplina)":
            return plots.discipline_time_performance(models, questions, False)
        case _:
            plots.model_performance(table)

def show_metrics(models,questions):    
    selected_models = st.multiselect("Selecione os modelo desejados: ", models, default=models[0], max_selections=5, key='multi' + '-'.join(models))
    
    table = utils.test_table(questions=questions,models=selected_models)
    
    st.dataframe(utils.format_test_table(table))
    
    plot = show_table_metrics(selected_models, questions, table)
    st.pyplot(plot)


def render(**kwargs):
    questoes = kwargs['questoes']
    
    st.title("üñ•Ô∏è Fase 1: Teste de Modelos Locais")

    st.markdown("Antes de definirmos qual modelo utilizar em nosso software, decidimos, como primeiro passo, testar modelos open-source dispon√≠veis. O objetivo foi compreender melhor a din√¢mica desses modelos e sua aplicabilidade em diferentes cen√°rios. Para isso, criamos um benchmark abrangente, avaliando uma s√©rie de modelos com diferentes quantidades de par√¢metros, tipos de quest√µes e analisando seu desempenho em diversas disciplinas.")

    st.subheader("1. Modelos")
    
    st.markdown("""Vamos explorar inicialmente modelos que podem ser obtidos utilizando o [Ollama](https://ollama.com/), um reposit√≥rio de v√°rios modelos de LLM, de forma similar ao Hugging Face, no qual baixamos localmente 20 modelos, variando em alguns a quantidade de par√¢metros, os quais podem ser modelos de reasoning, vis√£o computacional, focados em matem√°tica ou apenas texto.""")
    
    models_dataframe = pd.DataFrame(models_data)
    
    st.dataframe(
        models_dataframe.drop(columns=["Imagem", "Descri√ß√£o"]),
        column_config={
            "Imagem" : st.column_config.ImageColumn(width="medium"),
        },
        use_container_width=True
    )
    
    st.markdown("""
                ### 1.1 Vis√£o Geral dos Modelos
                Abaixo uma leve descri√ß√£o de cada um dos modelos utilizados""")
    
    for i, model in enumerate(models_dataframe.to_dict(orient="records"), start=1):
        st.markdown(f"#### 1.1.{i} {model['Modelo']}")
        
        cm1, cm2, cm3 = st.columns([2,3,2])
        cm2.image(model['Imagem'])
        
        st.markdown(f"""
            > Par√¢metros: {' '.join(map(lambda x : f'`{x}`', model['Par√¢metros']))} Tamanho: {' '.join(map(lambda x : f'`{x}GB`', model['Tamanho (Em GB)']))} Algoritmo : `{model['Algoritmo']}`
            
            {model['Descri√ß√£o']}
            """)
        
    
    st.markdown("""
                ### 1.2 Correla√ß√£o entre Par√¢metros e Tamanho
                Abaixo uma compara√ß√£o entre o tamanho do modelo (em GB) e a quantidade de par√¢metros (em bilh√µes)""")
    
    st.pyplot(utils.models_info.plot_parameters_x_size())
    
    st.markdown("""
               Observamos que, de maneira geral, **a rela√ß√£o entre a quantidade de par√¢metros e o tamanho do modelo segue um padr√£o bastante linear**. No entanto, h√° algumas diferen√ßas dependendo do tipo de modelo:  

                - **Modelos de reasoning** tendem a ser **ligeiramente maiores** do que outros modelos com a mesma quantidade de par√¢metros.  
                - **Modelos de vis√£o computacional (vision)** tamb√©m apresentam um **aumento sutil no tamanho**, possivelmente devido √†s camadas especializadas para processamento de imagens.  

                Essa an√°lise inicial nos permite compreender melhor as exig√™ncias de cada modelo em termos de armazenamento e mem√≥ria, ajudando na escolha da melhor op√ß√£o para diferentes aplica√ß√µes locais.
               """)
    
    st.subheader("2. Quest√µes de Texto")
    
    st.markdown("""
                ## 2. Quest√µes de Texto  
                
                O primeiro tipo de quest√£o que vamos analisar s√£o as **quest√µes de texto**, que representam **63,6%** de todas as quest√µes do dataset. Para avaliar o desempenho dos modelos nesse tipo de tarefa, dividimos os testes em **tr√™s grupos distintos**:  
                
                1. **Modelos com diferentes vers√µes de tokens** ‚Äì investigamos o impacto do tamanho do modelo no desempenho, comparando vers√µes de um mesmo modelo com quantidades variadas de tokens.  
                2. **Modelos com suporte a reasoning** ‚Äì testamos modelos projetados para racioc√≠nio avan√ßado, que, apesar de mais lentos, prometem um processamento mais elaborado e preciso.  
                3. **Demais modelos baseados apenas em texto** ‚Äì avaliamos modelos que lidam exclusivamente com entrada textual, sem otimiza√ß√µes espec√≠ficas para reasoning.  
                
                Para conduzir esse experimento, selecionamos **100 quest√µes de forma pseudo-aleat√≥ria**, garantindo uma distribui√ß√£o balanceada com **25 quest√µes de cada disciplina**. Dessa forma, buscamos assegurar que os resultados reflitam de maneira justa a capacidade dos modelos em interpretar e responder diferentes tipos de perguntas textuais.  
            """)
    
    
    # Quest√µes utilizadas no teste
    example_text_questions  = [2011013, 2009066, 2015026, 2014032, 2013088, 2011042, 2010041, 2021090, 2010028, 2023051, 2019054, 2009074, 2009071, 2010033, 2013006, 2014002, 2021048, 2023063, 2009068, 2022062, 2013027, 2019051, 2013063, 2019084, 2023050, 2015058, 2009026, 2012064, 2018134, 2016071, 2012062, 2013079, 2016069, 2011057, 2011051, 2017132, 2011053, 2017104, 2016080, 2023107, 2014089, 2010051, 2019133, 2021118, 2011082, 2017131, 2010081, 2021097, 2015072, 2023099, 2020034, 2016103, 2019044, 2013111, 2022017, 2011103, 2009133, 2021023, 2013099, 2023020, 2014099, 2011109, 2014102, 2011126, 2016115, 2014130, 2017031, 2020045, 2016104, 2012129, 2016108, 2015099, 2013123, 2021030, 2014123, 2021165, 2021142, 2020138, 2009179, 2019153, 2019170, 2011166, 2018161, 2022139, 2013139, 2011162, 2016174, 2015151, 2013166, 2019173, 2021158, 2018169, 2012171, 2021146, 2014168, 2022165, 2022169, 2009175, 2012176, 2009161]

    st.markdown("""
                ### 2.1 Avaliando o Impacto do Tamanho do Modelo  

                Para esse teste, utilizaremos **dois modelos principais**:  

                - **Qwen2.5**, nas vers√µes:  
                  - `14b` (9.0GB)  
                  - `7b` (4.7GB)  
                  - `1.5b` (1.9GB)  

                - **Gemma2**, nas vers√µes:  
                  - `27b` (16GB)  
                  - `9b` (5.4GB)  
                  - `2b` (1.6GB)  

                Escolhemos esses dois modelos porque, entre os selecionados, s√£o os que oferecem **maior diversidade de quantidade de par√¢metros**, permitindo uma an√°lise mais abrangente. Al√©m disso, eles apresentam uma varia√ß√£o de tamanho que cobre desde **modelos extremamente compactos** (menos de 2GB, ideais para sistemas embarcados) at√© **modelos mais convencionais** (`7b` e `9b`), al√©m de **op√ß√µes mais robustas**, como o `14b` e `27b`.  

                Para esse experimento, utilizaremos o mesmo **conjunto de quest√µes selecionadas de forma pseudo-aleat√≥ria**, j√° descrito anteriormente. Avaliaremos:  

                - **M√©tricas de desempenho**, incluindo n√∫mero de acertos e erros, tanto no geral quanto por disciplina.  
                - **Compara√ß√£o entre os dois modelos**, buscando identificar **tend√™ncias de efici√™ncia** em rela√ß√£o √† quantidade de par√¢metros.  

                Nosso objetivo final √© tentar inferir **at√© que ponto o aumento no n√∫mero de par√¢metros impacta o desempenho** e se h√° um ponto de equil√≠brio entre **qualidade da resposta e efici√™ncia computacional**.  
                """)
    
    st.markdown("""
                #### 2.1.1 - Qwen2.5  

                Nosso primeiro alvo de an√°lise √© o **Qwen 2.5**, avaliando seu desempenho em diferentes tamanhos e identificando padr√µes de comportamento.  

                ##### **Acur√°cia dos Modelos**  

                Ao analisarmos a **acur√°cia**, percebemos que **o n√∫mero de acertos se manteve est√°vel** entre o modelo mais robusto e o modelo intermedi√°rio. No entanto, ao compararmos com a vers√£o mais leve, observamos uma **queda significativa**, reduzindo-se para **menos da metade** da acur√°cia dos demais modelos.  

                ##### **Tempo de Execu√ß√£o**  

                O mesmo padr√£o foi identificado em rela√ß√£o ao **tempo de execu√ß√£o**. O modelo intermedi√°rio apresentou **tempos m√°ximos semelhantes ao modelo maior**, enquanto o modelo mais leve teve uma redu√ß√£o proporcional no tempo de infer√™ncia. Al√©m disso, ao analisarmos os tempos m√©dios, identificamos uma **rela√ß√£o linear entre a quantidade de par√¢metros e o tempo de execu√ß√£o** ‚Äì quanto menor o n√∫mero de par√¢metros, menor tende a ser o tempo necess√°rio para processar uma resposta.  

                ##### **Desempenho por Disciplina**  

                Ao compararmos o desempenho por disciplina, observamos que a **diferen√ßa entre o modelo intermedi√°rio e o modelo maior foi m√≠nima**, exceto por um leve aumento na acur√°cia em **ci√™ncias humanas** no modelo maior. Em **matem√°tica**, ambos os modelos apresentaram **o mesmo desempenho**, sem vantagens claras entre eles.  

                Um padr√£o recorrente em **todos os tamanhos do Qwen 2.5** foi a **baixa acur√°cia em quest√µes matem√°ticas**, o que sugere uma limita√ß√£o espec√≠fica do modelo nessa √°rea. Esse comportamento tamb√©m foi observado em outros modelos, indicando que pode ser uma caracter√≠stica comum entre os LLMs testados.  

                ##### **Visualiza√ß√£o dos Resultados**  

                Os gr√°ficos abaixo ilustram esses padr√µes. Caso esteja acessando via plataforma interativa, voc√™ pode **selecionar os modelos e o tipo de gr√°fico** para compara√ß√£o personalizada.
                """)
    
    show_metrics(["qwen2.5:14b", "qwen2.5:7b", "qwen2.5:1.5b"], example_text_questions)
    
    st.markdown("""
                #### 2.1.2 - Gemma2  

                Nesta se√ß√£o, analisamos o desempenho do modelo **Gemma2** em diferentes configura√ß√µes de tamanho, a fim de identificar padr√µes de comportamento e avaliar sua efici√™ncia em rela√ß√£o √† acur√°cia e ao tempo de execu√ß√£o.  

                ##### **Acur√°cia dos Modelos**  

                A an√°lise da **acur√°cia** revelou que, de maneira similar aos modelos **Qwen2.5**, a quantidade de acertos obtida pelo modelo mais robusto e pelo modelo intermedi√°rio permaneceu praticamente inalterada. No entanto, ao comparar com o modelo de menor porte, observou-se uma redu√ß√£o no n√∫mero de respostas corretas.  

                Diferentemente do que foi constatado no **Qwen2.5**, a perda de desempenho no modelo mais leve do **Gemma2** foi **menos acentuada**. A diferen√ßa observada foi de **apenas 10 respostas corretas a menos em rela√ß√£o ao modelo intermedi√°rio**, representando uma das melhores rela√ß√µes entre **desempenho e tamanho do modelo** entre os modelos avaliados.  

                ##### **Tempo de Execu√ß√£o**  

                Em rela√ß√£o ao **tempo de infer√™ncia**, os resultados obtidos n√£o seguiram a correla√ß√£o linear observada em outros modelos. O **modelo intermedi√°rio apresentou o menor tempo m√©dio e m√≠nimo de execu√ß√£o**, o que diverge da expectativa de um comportamento proporcional ao n√∫mero de par√¢metros. Esse fen√¥meno sugere que o **Gemma2 pode possuir otimiza√ß√µes internas**, que resultam em um melhor aproveitamento computacional em determinados tamanhos.  

                ##### **Desempenho por Disciplina**  

                A an√°lise do desempenho por disciplina revelou uma **intercala√ß√£o entre os modelos quanto √† sua efic√°cia em diferentes √°reas do conhecimento**:  

                - **Ci√™ncias humanas** ‚Äì Desempenho equivalente entre os modelos maiores, o menor ainda sim conseguiu uma quantidade de acertos aceit√°veis, variando em apenas 2 quest√µes.  
                - **Ci√™ncias naturais e matem√°tica** ‚Äì O modelo mais robusto apresentou um desempenho superior, com um aumento de **3 e 2 respostas corretas**, respectivamente.  
                - **Linguagens** ‚Äì O modelo intermedi√°rio superou o modelo mais robusto, obtendo **4 respostas corretas adicionais**.  

                Embora a **acur√°cia geral tenha se mantido semelhante entre os modelos**, a an√°lise segmentada por disciplina evidencia que **o desempenho dos modelos n√£o √© uniforme** e pode variar conforme o dom√≠nio da quest√£o analisada.  

                ##### **Desempenho em Matem√°tica**  

                O padr√£o de **baixo desempenho em matem√°tica** tamb√©m foi identificado no modelo **Gemma2**, confirmando a tend√™ncia observada em outros modelos. Especificamente, os resultados foram:  

                - **Modelo robusto** ‚Äì 7 acertos  
                - **Modelo intermedi√°rio** ‚Äì 5 acertos  
                - **Modelo leve** ‚Äì 4 acertos  

                Independentemente da configura√ß√£o utilizada, o desempenho em matem√°tica manteve-se reduzido, o que sugere uma **limita√ß√£o inerente ao modelo** no processamento desse tipo espec√≠fico de quest√£o.  

                ##### **Visualiza√ß√£o dos Resultados**  

                As informa√ß√µes detalhadas podem ser observadas nos gr√°ficos apresentados a seguir. Al√©m disso, caso o leitor esteja acessando por meio da plataforma interativa, √© poss√≠vel **selecionar os modelos e o tipo de gr√°fico desejado** para uma an√°lise comparativa personalizada.  
                """)
    
    show_metrics(["gemma2:2b", "gemma2", "gemma2:27b"], example_text_questions)
    
    st.markdown("""
                #### 2.1.3 - An√°lise Comparativa dos Modelos  

                Com base nos dados apresentados, observamos que h√° uma **diferen√ßa significativa** de desempenho entre os modelos mais robustos e os modelos mais leves. No entanto, ao compararmos um modelo **intermedi√°rio** com um modelo mais robusto, essa diferen√ßa torna-se **pouco expressiva**, sugerindo que modelos intermedi√°rios podem oferecer um desempenho equivalente na maioria dos cen√°rios. No caso do **Gemma2**, identificamos uma varia√ß√£o entre os acertos por disciplina, com **diferen√ßas mais not√°veis entre os modelos**, o que indica que alguns dom√≠nios podem ser mais sens√≠veis √† quantidade de par√¢metros do modelo.  

                ##### **Impacto Temporal e Considera√ß√µes Computacionais**  

                Em termos de **tempo de infer√™ncia**, o modelo mais robusto apresentou **tempos de execu√ß√£o mais elevados** em todos os testes. Esse comportamento pode ser explicado por dois fatores principais:  

                1. **Consumo de mem√≥ria** ‚Äì Modelos maiores tendem a exigir mais mem√≥ria, sendo preferencialmente executados em **mem√≥ria de v√≠deo (VRAM)** para maior efici√™ncia. No entanto, se o modelo ultrapassar a capacidade da GPU dispon√≠vel, ele ser√° carregado na **mem√≥ria RAM**, que possui uma **frequ√™ncia menor**, impactando diretamente no tempo de infer√™ncia.  
                2. **N√∫mero de combina√ß√µes avaliadas** ‚Äì Modelos com maior n√∫mero de par√¢metros possuem **mais possibilidades de ajuste para cada predi√ß√£o**, o que pode levar a um aumento no tempo de resposta.  

                Esses fatores refor√ßam a necessidade de um equil√≠brio entre **qualidade da resposta e viabilidade computacional**, especialmente em aplica√ß√µes que demandam tempo de resposta reduzido e menor consumo de recursos.  
                ##### **Formula√ß√£o e Teste da Hip√≥tese Nula**  
                """)
    
    st.subheader("Formula√ß√£o e Teste da Hip√≥tese Nula")

    st.write("Para avaliar a equival√™ncia entre modelos intermedi√°rios e robustos, formulamos a seguinte hip√≥tese estat√≠stica:")

    st.latex(r"H_0: \mu_{\text{intermedi√°rio}} = \mu_{\text{robusto}}")

    st.write("Ou seja, a **m√©dia de acertos do modelo intermedi√°rio** ("
             r"$\mu_{\text{intermedi√°rio}}$) √© estatisticamente equivalente √† "
             r"**m√©dia de acertos do modelo robusto** ($\mu_{\text{robusto}}$), "
             "indicando que ambos possuem desempenhos similares.")

    st.write("**Hip√≥tese Alternativa ($H_1$):**")

    st.latex(r"H_1: \mu_{\text{intermedi√°rio}} \neq \mu_{\text{robusto}}")

    st.write("Essa hip√≥tese alternativa indicaria que existe uma **diferen√ßa estatisticamente significativa** "
             "entre os modelos, sugerindo que um deles apresenta um desempenho superior ao outro.")

    st.subheader("Resultados dos Testes Estat√≠sticos")

    st.write("Para validar essa hip√≥tese, aplicamos **dois testes estat√≠sticos**:")
    st.write("- **Teste t de Student**, que avalia se h√° uma diferen√ßa significativa entre os modelos.")
    st.write("- **Teste de Equival√™ncia (TOST)**, que verifica se a diferen√ßa de desempenho entre os modelos "
             "est√° dentro de um intervalo previamente definido como aceit√°vel.")

    st.write("Os resultados obtidos foram os seguintes:")

    st.write("**Teste t de Student ($\\alpha = 0.05$):**")
    st.write("- Estat√≠stica t: **-0.1597**")
    st.write("- Valor-p: **0.8732**")
    st.write("- **Conclus√£o**: Como $p > 0.05$, aceitamos $H_0$, indicando que **n√£o h√° diferen√ßa estatisticamente significativa** entre os modelos intermedi√°rio e robusto.")

    st.write("**Teste de Equival√™ncia (TOST) com intervalo de $[-0.02, 0.02]$:**")
    st.write("- Valor-p: **0.3162**")
    st.write("- **Conclus√£o**: Como $p > 0.05$, **n√£o h√° evid√™ncias suficientes para afirmar equival√™ncia** dentro do intervalo estabelecido.")

    st.subheader("Interpreta√ß√£o dos Resultados")

    st.write("Os resultados obtidos demonstram que **os modelos intermedi√°rio e robusto possuem desempenhos estatisticamente similares**, "
             "pois o teste t de Student **n√£o encontrou diferen√ßa significativa** entre eles. "
             "No entanto, o teste de equival√™ncia **n√£o conseguiu confirmar que os modelos est√£o dentro do intervalo de toler√¢ncia de ¬±2%**, "
             "o que significa que **a equival√™ncia estat√≠stica n√£o pode ser garantida dentro desse crit√©rio espec√≠fico**.")

    st.write("Dessa forma, podemos concluir que **o aumento no n√∫mero de par√¢metros n√£o implica necessariamente em uma melhoria significativa no desempenho**, "
             "mas tamb√©m que **n√£o podemos afirmar que um modelo intermedi√°rio √© equivalente a um robusto dentro da margem de erro estipulada**.")

    st.write("Na pr√°tica, a escolha entre um modelo intermedi√°rio ou robusto deve considerar n√£o apenas a acur√°cia estat√≠stica, "
             "mas tamb√©m **fatores como tempo de infer√™ncia, consumo de mem√≥ria e viabilidade computacional**, "
             "uma vez que modelos intermedi√°rios podem oferecer benef√≠cios significativos em efici√™ncia sem comprometer substancialmente a qualidade das respostas.")

    
    st.markdown("""
                ### 2.2 - Avalia√ß√£o de Modelos de Reasoning  

                Nesta se√ß√£o, analisamos o desempenho dos **modelos de Reasoning**, que, devido √† sua estrutura avan√ßada de processamento e racioc√≠nio, s√£o esperados apresentar **melhor desempenho** em tarefas que exigem l√≥gica e c√°lculos mais complexos. Para esse experimento, utilizamos os seguintes modelos: **Deepscaler**, **Deepseek-r1**, **Mistral-nemo**, **Openthinker** e **Smallthinker** .

                ##### **Tempo de Execu√ß√£o e Uso de Tokens**  

                Observamos uma **diferen√ßa significativa no tempo de infer√™ncia** desses modelos em compara√ß√£o com os testados anteriormente. Alguns deles apresentaram a tag `<think>`, que representa uma etapa expl√≠cita de racioc√≠nio antes da resposta final. Essa estrutura resultou em **respostas mais longas**, com um maior n√∫mero de **tokens**, o que impactou diretamente no tempo de processamento.  

                Um caso extremo foi o **Openthinker**, que levou **mais de 1h24min** para processar **100 quest√µes**, resultando em uma m√©dia de aproximadamente **50 segundos por quest√£o**. Em contraste, o modelo **Gemma2:27b**, que anteriormente apresentou o maior tempo de execu√ß√£o entre os modelos testados, levou **cerca de 12 minutos** para concluir o mesmo teste.  

                Vale ressaltar que, diferentemente do **Gemma2:27b**, cujo tempo elevado se deve ao alto consumo de **VRAM**, os modelos de reasoning s√£o **mais leves em tamanho**, sugerindo que o tempo prolongado pode estar mais relacionado ao **processamento adicional de racioc√≠nio**. Essa tend√™ncia foi observada em **todos os modelos**, exceto pelo **Mistral-nemo**, que conseguiu um tempo de infer√™ncia reduzido. 

                Um outro fator importante foi o excesso de **timeout**, com os demais modelos n√£o tivemos problemas, mas o **smallthinker** apresentou uma incost√¢ncia muito grande, travando a execu√ß√£o, tendo que ser implementado um timeout para impedir que ele interrompesse os teste.

                ##### **Acur√°cia e Desempenho por Disciplina**  

                Em rela√ß√£o √†s **m√©tricas de acur√°cia**, os resultados foram variados:  

                - Apenas o **Deepscaler** apresentou um desempenho **inferior** aos modelos convencionais testados anteriormente.  
                - Os demais modelos tiveram valores **pr√≥ximos** aos obtidos pelos modelos **n√£o-reasoning**.  
                - Em **matem√°tica**, esperava-se que os modelos reasoning apresentassem **um desempenho superior**, devido √† sua **capacidade aprimorada de c√°lculos**. No entanto, apenas o **Deepseek-r1** obteve **mais de 50% de acertos** nessa disciplina, enquanto nas demais √°reas seu desempenho foi inferior, especialmente em **ci√™ncias da natureza**.  

                ##### **Respostas Nulas e Impacto no N√£o-Determinismo**  

                Um fen√¥meno observado nesses modelos foi a **alta taxa de respostas nulas**, especialmente em **matem√°tica**. Respostas nulas ocorreram quando o modelo **n√£o seguiu corretamente a instru√ß√£o de resposta**, por exemplo:  

                - Em vez de fornecer a **alternativa correta**, o modelo apresentava o **resultado do c√°lculo**.  
                - Algumas respostas continham explica√ß√µes extensas, mas sem indicar diretamente a alternativa correta.  

                Esse comportamento indica que a **camada adicional de racioc√≠nio pode impactar o determinismo do modelo**, tornando suas respostas menos previs√≠veis e mais propensas a **desvios das instru√ß√µes originais**.  

                ##### **Considera√ß√µes Finais**  

                Os modelos de reasoning apresentaram **vantagens e desvantagens** claras. Enquanto alguns conseguiram **bons desempenhos em acur√°cia**, houve um **custo expressivo em tempo de infer√™ncia**. Al√©m disso, a **tend√™ncia a respostas mais elaboradas e menos diretas** pode ser um desafio em aplica√ß√µes que exigem **alta confiabilidade e ader√™ncia estrita √†s instru√ß√µes**.                  
                """)
    
    show_metrics(["deepscaler", "deepseek-r1", "mistral-nemo", "openthinker", "smallthinker"], example_text_questions)
    
    
    
    
    
    
    
    