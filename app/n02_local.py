import streamlit as st
import lib.utils as utils
from lib.utils import plots
import pandas as pd
import random
from lib.utils.models_info import models_data
random.seed(42)


def show_table_metrics(models, questions, table):
    table_type = [
        "Desempenho de Acerto x Erros", "M√©tricas de Tempo por Modelo", "Tempo Total por Modelo",
        "Correla√ß√£o Tamanho x Acur√°cia", "Correla√ß√£o Tempo M√©dio x Acur√°cia", "Correla√ß√£o Tempo x Tamanho",
        "Desempenho por Disciplina (Agrupado por Modelo)", "Desempenho por Disciplina (Agrupado por Disciplina)",
        "Tempo por Disciplina (Agrupado por Modelo)", "Tempo por Disciplina (Agrupado por Disciplina)",
        "Diagrama de Venn"
    ]
    
    plot_type = st.selectbox("Seelcione uma forma de visualiza√ß√£o: ", table_type, index=0, key='unic' + '-'.join(models))
    
    match(plot_type):
        case "Desempenho de Acerto x Erros":
            return plots.model_performance(table)
        case "M√©tricas de Tempo por Modelo":
            return plots.time_metrics(table)
        case "Tempo Total por Modelo":
            return plots.time_metrics_total(table)
        case "Correla√ß√£o Tamanho x Acur√°cia":
            return plots.correlation(table, x="Size", y="Acc")
        case "Correla√ß√£o Tempo M√©dio x Acur√°cia":
            return plots.correlation(table, x="Tavg", y="Acc")
        case "Correla√ß√£o Tempo x Tamanho":
            return plots.correlation(table, x="Tavg", y="Size")
        case "Desempenho por Disciplina (Agrupado por Modelo)":
            return plots.discipline_performance(models, questions,True, False)
        case "Desempenho por Disciplina (Agrupado por Disciplina)":
            return plots.discipline_performance(models, questions, False, False)
        case "Tempo por Disciplina (Agrupado por Modelo)":
            return plots.discipline_time_performance(models, questions, True)
        case "Tempo por Disciplina (Agrupado por Disciplina)":
            return plots.discipline_time_performance(models, questions, False)
        case "Diagrama de Venn":
            if len(questions) < 2:
                st.info("Voc√™ precisa selecionar pelo menos duas disciplinas para visualizar o diagrama de Venn.")
                return None
            if len(questions) > 3:
                st.info("Voc√™ pode escolher at√© 3 disciplinas para visualizar o diagrama de Venn.")
                return None
            return plots.venn_diagram(models, *questions)
        case _:
            plots.model_performance(table)

def show_metrics(models,questions):    
    selected_models = st.multiselect("Selecione os modelo desejados: ", models, default=models[0], max_selections=5, key='multi' + '-'.join(models))
    
    table = utils.test_table(questions=questions,models=selected_models)
    
    st.dataframe(utils.format_test_table(table))
    
    plot = show_table_metrics(selected_models, questions, table)
    st.pyplot(plot)


def show_discipline_metrics(table, discipline):
    
    plot_type = st.selectbox("Seelcione uma forma de visualiza√ß√£o: ", ["Quest√µes Acertadas", "Tempo M√©dio"], index=0, key=discipline)
    
    st.pyplot(plots.discipline_models(table, discipline, 'acc' if plot_type == "Quest√µes Acertadas" else 'time'))


def render(**kwargs):
    questoes = kwargs['questoes']
    
    st.title("üñ•Ô∏è Fase 1: Teste de Modelos Locais")

    st.markdown("Antes de definirmos qual modelo utilizar em nosso software, decidimos, como primeiro passo, testar modelos open-source dispon√≠veis. O objetivo foi compreender melhor a din√¢mica desses modelos e sua aplicabilidade em diferentes cen√°rios. Para isso, criamos um benchmark abrangente, avaliando uma s√©rie de modelos com diferentes quantidades de par√¢metros, tipos de quest√µes e analisando seu desempenho em diversas disciplinas.")

    st.subheader("1. Modelos")
    
    st.markdown("""Vamos explorar inicialmente modelos que podem ser obtidos utilizando o [Ollama](https://ollama.com/), um reposit√≥rio de v√°rios modelos de LLM, de forma similar ao Hugging Face, no qual baixamos localmente 20 modelos, variando em alguns a quantidade de par√¢metros, os quais podem ser modelos de reasoning, vis√£o computacional, focados em matem√°tica ou apenas texto.""")
    
    models_dataframe = pd.DataFrame(models_data)
    
    st.dataframe(
        models_dataframe.drop(columns=["Imagem", "Descri√ß√£o"]),
        column_config={
            "Imagem" : st.column_config.ImageColumn(width="medium"),
        },
        use_container_width=True
    )
    
    st.markdown("""
                ### 1.1 Vis√£o Geral dos Modelos
                Abaixo uma leve descri√ß√£o de cada um dos modelos utilizados""")
    
    for i, model in enumerate(models_dataframe.to_dict(orient="records"), start=1):
        st.markdown(f"#### 1.1.{i} {model['Modelo']}", )
        
        cm1, cm2 = st.columns([2,3], vertical_alignment='center')
        cm1.image(model['Imagem'])
        cm2.markdown(f"""
            > Par√¢metros: {' '.join(map(lambda x : f'`{x}`', model['Par√¢metros']))} Tamanho: {' '.join(map(lambda x : f'`{x}GB`', model['Tamanho (Em GB)']))} Algoritmo : `{model['Algoritmo']}`
            
            {model['Descri√ß√£o']}
            """)
        
    
    st.markdown("""
                ### 1.2 Correla√ß√£o entre Par√¢metros e Tamanho
                Abaixo uma compara√ß√£o entre o tamanho do modelo (em GB) e a quantidade de par√¢metros (em bilh√µes)""")
    
    st.pyplot(utils.models_info.plot_parameters_x_size())
    
    st.markdown("""
               Observamos que, de maneira geral, **a rela√ß√£o entre a quantidade de par√¢metros e o tamanho do modelo segue um padr√£o bastante linear**. No entanto, h√° algumas diferen√ßas dependendo do tipo de modelo:  

                - **Modelos de reasoning** tendem a ser **ligeiramente maiores** do que outros modelos com a mesma quantidade de par√¢metros.  
                - **Modelos de vis√£o computacional (vision)** tamb√©m apresentam um **aumento sutil no tamanho**, possivelmente devido √†s camadas especializadas para processamento de imagens.  

                Essa an√°lise inicial nos permite compreender melhor as exig√™ncias de cada modelo em termos de armazenamento e mem√≥ria, ajudando na escolha da melhor op√ß√£o para diferentes aplica√ß√µes locais.
               """)
        
    st.markdown("""
                ## 2. Quest√µes de Texto  
                
                O primeiro tipo de quest√£o que vamos analisar s√£o as **quest√µes de texto**, que representam **63,6%** de todas as quest√µes do dataset. Para avaliar o desempenho dos modelos nesse tipo de tarefa, dividimos os testes em **tr√™s grupos distintos**:  
                
                1. **Modelos com diferentes vers√µes de tokens** ‚Äì investigamos o impacto do tamanho do modelo no desempenho, comparando vers√µes de um mesmo modelo com quantidades variadas de tokens.  
                2. **Modelos com suporte a reasoning** ‚Äì testamos modelos projetados para racioc√≠nio avan√ßado, que, apesar de mais lentos, prometem um processamento mais elaborado e preciso.  
                3. **Demais modelos baseados apenas em texto** ‚Äì avaliamos modelos que lidam exclusivamente com entrada textual, sem otimiza√ß√µes espec√≠ficas para reasoning.  
                
                Para conduzir esse experimento, selecionamos **100 quest√µes de forma pseudo-aleat√≥ria**, garantindo uma distribui√ß√£o balanceada com **25 quest√µes de cada disciplina**. Dessa forma, buscamos assegurar que os resultados reflitam de maneira justa a capacidade dos modelos em interpretar e responder diferentes tipos de perguntas textuais.  
            """)
    
    
    # Quest√µes utilizadas no teste
    example_text_questions  = [2011013, 2009066, 2015026, 2014032, 2013088, 2011042, 2010041, 2021090, 2010028, 2023051, 2019054, 2009074, 2009071, 2010033, 2013006, 2014002, 2021048, 2023063, 2009068, 2022062, 2013027, 2019051, 2013063, 2019084, 2023050, 2015058, 2009026, 2012064, 2018134, 2016071, 2012062, 2013079, 2016069, 2011057, 2011051, 2017132, 2011053, 2017104, 2016080, 2023107, 2014089, 2010051, 2019133, 2021118, 2011082, 2017131, 2010081, 2021097, 2015072, 2023099, 2020034, 2016103, 2019044, 2013111, 2022017, 2011103, 2009133, 2021023, 2013099, 2023020, 2014099, 2011109, 2014102, 2011126, 2016115, 2014130, 2017031, 2020045, 2016104, 2012129, 2016108, 2015099, 2013123, 2021030, 2014123, 2021165, 2021142, 2020138, 2009179, 2019153, 2019170, 2011166, 2018161, 2022139, 2013139, 2011162, 2016174, 2015151, 2013166, 2019173, 2021158, 2018169, 2012171, 2021146, 2014168, 2022165, 2022169, 2009175, 2012176, 2009161]

    st.markdown("""
                ### 2.1 Avaliando o Impacto do Tamanho do Modelo  

                Para esse teste, utilizaremos **dois modelos principais**, o **Qwen2.5**, nas vers√µes: `14b` (9.0GB), `7b` (4.7GB) e `1.5b` (1.9GB), como tamb√©m o **Gemma2**, nas vers√µes `27b` (16GB), `9b` (5.4GB) e`2b` (1.6GB). Escolhemos esses dois modelos porque, entre os selecionados, s√£o os que oferecem **maior diversidade de quantidade de par√¢metros**, permitindo uma an√°lise mais abrangente. Al√©m disso, eles apresentam uma varia√ß√£o de tamanho que cobre desde **modelos extremamente compactos** (menos de 2GB, ideais para sistemas embarcados) at√© **modelos mais convencionais** (`7b` e `9b`), al√©m de **op√ß√µes mais robustas**, como o `14b` e `27b`.  

                Para esse experimento, utilizaremos o mesmo **conjunto de quest√µes selecionadas de forma pseudo-aleat√≥ria**, j√° descrito anteriormente. Avaliaremos:  

                - **M√©tricas de desempenho**, incluindo n√∫mero de acertos e erros, tanto no geral quanto por disciplina.  
                - **Compara√ß√£o entre os dois modelos**, buscando identificar **tend√™ncias de efici√™ncia** em rela√ß√£o √† quantidade de par√¢metros.  

                Nosso objetivo final √© tentar inferir **at√© que ponto o aumento no n√∫mero de par√¢metros impacta o desempenho** e se h√° um ponto de equil√≠brio entre **qualidade da resposta e efici√™ncia computacional**.  
                """)
    
    st.markdown("""
                #### 2.1.1 - Qwen2.5  

                Nosso primeiro alvo de an√°lise √© o **Qwen 2.5**, avaliando seu desempenho em diferentes tamanhos e identificando padr√µes de comportamento.  

                ##### **Acur√°cia dos Modelos**  

                Ao analisarmos a **acur√°cia**, percebemos que **o n√∫mero de acertos se manteve est√°vel** entre o modelo mais robusto e o modelo intermedi√°rio. No entanto, ao compararmos com a vers√£o mais leve, observamos uma **queda significativa**, reduzindo-se para **menos da metade** da acur√°cia dos demais modelos.  

                ##### **Tempo de Execu√ß√£o**  

                O mesmo padr√£o foi identificado em rela√ß√£o ao **tempo de execu√ß√£o**. O modelo intermedi√°rio apresentou **tempos m√°ximos semelhantes ao modelo maior**, enquanto o modelo mais leve teve uma redu√ß√£o proporcional no tempo de infer√™ncia. Al√©m disso, ao analisarmos os tempos m√©dios, identificamos uma **rela√ß√£o linear entre a quantidade de par√¢metros e o tempo de execu√ß√£o** ‚Äì quanto menor o n√∫mero de par√¢metros, menor tende a ser o tempo necess√°rio para processar uma resposta.  

                ##### **Desempenho por Disciplina**  

                Ao compararmos o desempenho por disciplina, observamos que a **diferen√ßa entre o modelo intermedi√°rio e o modelo maior foi m√≠nima**, exceto por um leve aumento na acur√°cia em **ci√™ncias humanas** no modelo maior. Em **matem√°tica**, ambos os modelos apresentaram **o mesmo desempenho**, sem vantagens claras entre eles.  

                Um padr√£o recorrente em **todos os tamanhos do Qwen 2.5** foi a **baixa acur√°cia em quest√µes matem√°ticas**, o que sugere uma limita√ß√£o espec√≠fica do modelo nessa √°rea. Esse comportamento tamb√©m foi observado em outros modelos, indicando que pode ser uma caracter√≠stica comum entre os LLMs testados.  

                ##### **Visualiza√ß√£o dos Resultados**  

                Os gr√°ficos abaixo ilustram esses padr√µes. Caso esteja acessando via plataforma interativa, voc√™ pode **selecionar os modelos e o tipo de gr√°fico** para compara√ß√£o personalizada.
                """)
    
    show_metrics(["qwen2.5:14b", "qwen2.5:7b", "qwen2.5:1.5b"], example_text_questions)
    
    st.markdown("""
                #### 2.1.2 - Gemma2  

                Nesta se√ß√£o, analisamos o desempenho do modelo **Gemma2** em diferentes configura√ß√µes de tamanho, a fim de identificar padr√µes de comportamento e avaliar sua efici√™ncia em rela√ß√£o √† acur√°cia e ao tempo de execu√ß√£o.  

                ##### **Acur√°cia dos Modelos**  

                A an√°lise da **acur√°cia** revelou que, de maneira similar aos modelos **Qwen2.5**, a quantidade de acertos obtida pelo modelo mais robusto e pelo modelo intermedi√°rio permaneceu praticamente inalterada. No entanto, ao comparar com o modelo de menor porte, observou-se uma redu√ß√£o no n√∫mero de respostas corretas.  

                Diferentemente do que foi constatado no **Qwen2.5**, a perda de desempenho no modelo mais leve do **Gemma2** foi **menos acentuada**. A diferen√ßa observada foi de **apenas 10 respostas corretas a menos em rela√ß√£o ao modelo intermedi√°rio**, representando uma das melhores rela√ß√µes entre **desempenho e tamanho do modelo** entre os modelos avaliados.  

                ##### **Tempo de Execu√ß√£o**  

                Em rela√ß√£o ao **tempo de infer√™ncia**, os resultados obtidos n√£o seguiram a correla√ß√£o linear observada em outros modelos. O **modelo intermedi√°rio apresentou o menor tempo m√©dio e m√≠nimo de execu√ß√£o**, o que diverge da expectativa de um comportamento proporcional ao n√∫mero de par√¢metros. Esse fen√¥meno sugere que o **Gemma2 pode possuir otimiza√ß√µes internas**, que resultam em um melhor aproveitamento computacional em determinados tamanhos.  

                ##### **Desempenho por Disciplina**  

                A an√°lise do desempenho por disciplina revelou uma **intercala√ß√£o entre os modelos quanto √† sua efic√°cia em diferentes √°reas do conhecimento**:  

                - **Ci√™ncias humanas** ‚Äì Desempenho equivalente entre os modelos maiores, o menor ainda sim conseguiu uma quantidade de acertos aceit√°veis, variando em apenas 2 quest√µes.  
                - **Ci√™ncias naturais e matem√°tica** ‚Äì O modelo mais robusto apresentou um desempenho superior, com um aumento de **3 e 2 respostas corretas**, respectivamente.  
                - **Linguagens** ‚Äì O modelo intermedi√°rio superou o modelo mais robusto, obtendo **4 respostas corretas adicionais**.  

                Embora a **acur√°cia geral tenha se mantido semelhante entre os modelos**, a an√°lise segmentada por disciplina evidencia que **o desempenho dos modelos n√£o √© uniforme** e pode variar conforme o dom√≠nio da quest√£o analisada.  

                ##### **Desempenho em Matem√°tica**  

                O padr√£o de **baixo desempenho em matem√°tica** tamb√©m foi identificado no modelo **Gemma2**, confirmando a tend√™ncia observada em outros modelos. Especificamente, os resultados foram:  

                - **Modelo robusto** ‚Äì 7 acertos  
                - **Modelo intermedi√°rio** ‚Äì 5 acertos  
                - **Modelo leve** ‚Äì 4 acertos  

                Independentemente da configura√ß√£o utilizada, o desempenho em matem√°tica manteve-se reduzido, o que sugere uma **limita√ß√£o inerente ao modelo** no processamento desse tipo espec√≠fico de quest√£o.  

                ##### **Visualiza√ß√£o dos Resultados**  

                As informa√ß√µes detalhadas podem ser observadas nos gr√°ficos apresentados a seguir. Al√©m disso, caso o leitor esteja acessando por meio da plataforma interativa, √© poss√≠vel **selecionar os modelos e o tipo de gr√°fico desejado** para uma an√°lise comparativa personalizada.  
                """)
    
    show_metrics(["gemma2:2b", "gemma2", "gemma2:27b"], example_text_questions)
    
    st.markdown("""
                #### 2.1.3 - An√°lise Comparativa dos Modelos  

                Com base nos dados apresentados, observamos que h√° uma **diferen√ßa significativa** de desempenho entre os modelos mais robustos e os modelos mais leves. No entanto, ao compararmos um modelo **intermedi√°rio** com um modelo mais robusto, essa diferen√ßa torna-se **pouco expressiva**, sugerindo que modelos intermedi√°rios podem oferecer um desempenho equivalente na maioria dos cen√°rios. No caso do **Gemma2**, identificamos uma varia√ß√£o entre os acertos por disciplina, com **diferen√ßas mais not√°veis entre os modelos**, o que indica que alguns dom√≠nios podem ser mais sens√≠veis √† quantidade de par√¢metros do modelo.  

                ##### **Impacto Temporal e Considera√ß√µes Computacionais**  

                Em termos de **tempo de infer√™ncia**, o modelo mais robusto apresentou **tempos de execu√ß√£o mais elevados** em todos os testes. Esse comportamento pode ser explicado por dois fatores principais:  

                1. **Consumo de mem√≥ria** ‚Äì Modelos maiores tendem a exigir mais mem√≥ria, sendo preferencialmente executados em **mem√≥ria de v√≠deo (VRAM)** para maior efici√™ncia. No entanto, se o modelo ultrapassar a capacidade da GPU dispon√≠vel, ele ser√° carregado na **mem√≥ria RAM**, que possui uma **frequ√™ncia menor**, impactando diretamente no tempo de infer√™ncia.  
                2. **N√∫mero de combina√ß√µes avaliadas** ‚Äì Modelos com maior n√∫mero de par√¢metros possuem **mais possibilidades de ajuste para cada predi√ß√£o**, o que pode levar a um aumento no tempo de resposta.  

                Esses fatores refor√ßam a necessidade de um equil√≠brio entre **qualidade da resposta e viabilidade computacional**, especialmente em aplica√ß√µes que demandam tempo de resposta reduzido e menor consumo de recursos.  
                ##### **Formula√ß√£o e Teste da Hip√≥tese Nula**  
                """)
    
    st.subheader("Formula√ß√£o e Teste da Hip√≥tese Nula")

    st.write("Para avaliar a equival√™ncia entre modelos intermedi√°rios e robustos, formulamos a seguinte hip√≥tese estat√≠stica:")

    st.latex(r"H_0: \mu_{\text{intermedi√°rio}} = \mu_{\text{robusto}}")

    st.write("Ou seja, a **m√©dia de acertos do modelo intermedi√°rio** ("
             r"$\mu_{\text{intermedi√°rio}}$) √© estatisticamente equivalente √† "
             r"**m√©dia de acertos do modelo robusto** ($\mu_{\text{robusto}}$), "
             "indicando que ambos possuem desempenhos similares.")

    st.write("**Hip√≥tese Alternativa ($H_1$):**")

    st.latex(r"H_1: \mu_{\text{intermedi√°rio}} \neq \mu_{\text{robusto}}")

    st.write("Essa hip√≥tese alternativa indicaria que existe uma **diferen√ßa estatisticamente significativa** "
             "entre os modelos, sugerindo que um deles apresenta um desempenho superior ao outro.")

    st.subheader("Resultados dos Testes Estat√≠sticos")

    st.write("Para validar essa hip√≥tese, aplicamos **dois testes estat√≠sticos**:")
    st.write("- **Teste t de Student**, que avalia se h√° uma diferen√ßa significativa entre os modelos.")
    st.write("- **Teste de Equival√™ncia (TOST)**, que verifica se a diferen√ßa de desempenho entre os modelos "
             "est√° dentro de um intervalo previamente definido como aceit√°vel.")

    st.write("Os resultados obtidos foram os seguintes:")

    st.write("**Teste t de Student ($\\alpha = 0.05$):**")
    st.write("- Estat√≠stica t: **-0.1597**")
    st.write("- Valor-p: **0.8732**")
    st.write("- **Conclus√£o**: Como $p > 0.05$, aceitamos $H_0$, indicando que **n√£o h√° diferen√ßa estatisticamente significativa** entre os modelos intermedi√°rio e robusto.")

    st.write("**Teste de Equival√™ncia (TOST) com intervalo de $[-0.02, 0.02]$:**")
    st.write("- Valor-p: **0.3162**")
    st.write("- **Conclus√£o**: Como $p > 0.05$, **n√£o h√° evid√™ncias suficientes para afirmar equival√™ncia** dentro do intervalo estabelecido.")

    st.subheader("Interpreta√ß√£o dos Resultados")

    st.write("Os resultados obtidos demonstram que **os modelos intermedi√°rio e robusto possuem desempenhos estatisticamente similares**, "
             "pois o teste t de Student **n√£o encontrou diferen√ßa significativa** entre eles. "
             "No entanto, o teste de equival√™ncia **n√£o conseguiu confirmar que os modelos est√£o dentro do intervalo de toler√¢ncia de ¬±2%**, "
             "o que significa que **a equival√™ncia estat√≠stica n√£o pode ser garantida dentro desse crit√©rio espec√≠fico**.")

    st.write("Dessa forma, podemos concluir que **o aumento no n√∫mero de par√¢metros n√£o implica necessariamente em uma melhoria significativa no desempenho**, "
             "mas tamb√©m que **n√£o podemos afirmar que um modelo intermedi√°rio √© equivalente a um robusto dentro da margem de erro estipulada**.")

    st.write("Na pr√°tica, a escolha entre um modelo intermedi√°rio ou robusto deve considerar n√£o apenas a acur√°cia estat√≠stica, "
             "mas tamb√©m **fatores como tempo de infer√™ncia, consumo de mem√≥ria e viabilidade computacional**, "
             "uma vez que modelos intermedi√°rios podem oferecer benef√≠cios significativos em efici√™ncia sem comprometer substancialmente a qualidade das respostas.")

    
    st.markdown("""
                ### 2.2 - Avalia√ß√£o de Modelos de Reasoning  

                Nesta se√ß√£o, analisamos o desempenho dos **modelos de Reasoning**, que, devido √† sua estrutura avan√ßada de processamento e racioc√≠nio, s√£o esperados apresentar **melhor desempenho** em tarefas que exigem l√≥gica e c√°lculos mais complexos. Para esse experimento, utilizamos os seguintes modelos: **Deepscaler**, **Deepseek-r1**, **Mistral-nemo**, **Openthinker** e **Smallthinker** .

                ##### **Tempo de Execu√ß√£o e Uso de Tokens**  

                Observamos uma **diferen√ßa significativa no tempo de infer√™ncia** desses modelos em compara√ß√£o com os testados anteriormente. Alguns deles apresentaram a tag `<think>`, que representa uma etapa expl√≠cita de racioc√≠nio antes da resposta final. Essa estrutura resultou em **respostas mais longas**, com um maior n√∫mero de **tokens**, o que impactou diretamente no tempo de processamento.  

                Um caso extremo foi o **Openthinker**, que levou **mais de 1h24min** para processar **100 quest√µes**, resultando em uma m√©dia de aproximadamente **50 segundos por quest√£o**. Em contraste, o modelo **Gemma2:27b**, que anteriormente apresentou o maior tempo de execu√ß√£o entre os modelos testados, levou **cerca de 12 minutos** para concluir o mesmo teste.  

                Vale ressaltar que, diferentemente do **Gemma2:27b**, cujo tempo elevado se deve ao alto consumo de **VRAM**, os modelos de reasoning s√£o **mais leves em tamanho**, sugerindo que o tempo prolongado pode estar mais relacionado ao **processamento adicional de racioc√≠nio**. Essa tend√™ncia foi observada em **todos os modelos**, exceto pelo **Mistral-nemo**, que conseguiu um tempo de infer√™ncia reduzido. 

                Um outro fator importante foi o excesso de **timeout**, com os demais modelos n√£o tivemos problemas, mas o **smallthinker** apresentou uma incost√¢ncia muito grande, travando a execu√ß√£o, tendo que ser implementado um timeout para impedir que ele interrompesse os teste.

                ##### **Acur√°cia e Desempenho por Disciplina**  

                Em rela√ß√£o √†s **m√©tricas de acur√°cia**, os resultados foram variados:  

                - Apenas o **Deepscaler** apresentou um desempenho **inferior** aos modelos convencionais testados anteriormente.  
                - Os demais modelos tiveram valores **pr√≥ximos** aos obtidos pelos modelos **n√£o-reasoning**.  
                - Em **matem√°tica**, esperava-se que os modelos reasoning apresentassem **um desempenho superior**, devido √† sua **capacidade aprimorada de c√°lculos**. No entanto, apenas o **Deepseek-r1** obteve **mais de 50% de acertos** nessa disciplina, enquanto nas demais √°reas seu desempenho foi inferior, especialmente em **ci√™ncias da natureza**.  

                ##### **Respostas Nulas e Impacto no N√£o-Determinismo**  

                Um fen√¥meno observado nesses modelos foi a **alta taxa de respostas nulas**, especialmente em **matem√°tica**. Respostas nulas ocorreram quando o modelo **n√£o seguiu corretamente a instru√ß√£o de resposta**, por exemplo:  

                - Em vez de fornecer a **alternativa correta**, o modelo apresentava o **resultado do c√°lculo**.  
                - Algumas respostas continham explica√ß√µes extensas, mas sem indicar diretamente a alternativa correta.  

                Esse comportamento indica que a **camada adicional de racioc√≠nio pode impactar o determinismo do modelo**, tornando suas respostas menos previs√≠veis e mais propensas a **desvios das instru√ß√µes originais**.  

                ##### **Considera√ß√µes Finais**  

                Os modelos de reasoning apresentaram **vantagens e desvantagens** claras. Enquanto alguns conseguiram **bons desempenhos em acur√°cia**, houve um **custo expressivo em tempo de infer√™ncia**. Al√©m disso, a **tend√™ncia a respostas mais elaboradas e menos diretas** pode ser um desafio em aplica√ß√µes que exigem **alta confiabilidade e ader√™ncia estrita √†s instru√ß√µes**.                  
                """)
    
    show_metrics(["deepscaler", "deepseek-r1", "mistral-nemo", "openthinker", "smallthinker"], example_text_questions)
    
    st.markdown("""
                ### 2.3 - Avalia√ß√£o de Modelos Focados em Matem√°tica  

                Nesta se√ß√£o, analisamos o desempenho de modelos especializados em matem√°tica, avaliando se sua otimiza√ß√£o para essa √°rea impacta significativamente a acur√°cia em diferentes disciplinas. Os modelos testados foram o **Mathstral**, uma varia√ß√£o do **Mistral 7B** com aprimoramento em ci√™ncias da natureza e matem√°tica, e o **Qwen2-Math**, uma vers√£o especializada do **Qwen2**, testada nas configura√ß√µes de **1.5B** e **7B** par√¢metros.  

                Ao comparar esses modelos com os testados anteriormente, observamos que sua acur√°cia geral foi inferior, sugerindo que, apesar do foco em matem√°tica, seu desempenho em outras disciplinas foi prejudicado. Em especial, notamos um desempenho bastante reduzido em linguagens, onde o maior n√∫mero de acertos registrado foi de apenas **11**, um resultado significativamente inferior ao dos modelos intermedi√°rios analisados previamente.  

                Al√©m da acur√°cia, analisamos tamb√©m o tempo de infer√™ncia. Os resultados demonstram que as duas vers√µes do **Qwen2-Math** apresentaram uma diferen√ßa m√≠nima em termos de acur√°cia, no entanto, a vers√£o **1.5B teve um tempo m√©dio de infer√™ncia muito superior** ao da vers√£o **7B**. J√° o **Mathstral**, que possui um tamanho similar ao **Qwen2-Math:7B**, levou **mais do que o dobro do tempo** para concluir o teste, o que indica que diferen√ßas arquiteturais podem ter impactado significativamente sua efici√™ncia computacional.  

                Embora a expectativa fosse de que esses modelos apresentassem um desempenho significativamente superior em matem√°tica, os resultados mostraram que essa melhora n√£o foi expressiva. O modelo que obteve o melhor desempenho na disciplina foi o **Mathstral7b**, por√©m, de forma surpreendente, o **Qwen2-Math:1.5B**, que, mesmo possuindo **uma quantidade muito inferior de par√¢metros**, superou o desempenho de sua varia√ß√£o de par√¢metros maior, e ainda conseguiu ser mais r√°pido como cidato anteriormente.  

                Diante desses resultados, √© poss√≠vel concluir que modelos especializados em matem√°tica **n√£o necessariamente garantem um desempenho superior na disciplina** e, em contrapartida, apresentam **quedas expressivas em outras √°reas**, tornando-os menos vers√°teis. Al√©m disso, a rela√ß√£o entre **tamanho do modelo, tempo de infer√™ncia e acur√°cia** nem sempre segue um padr√£o linear, como evidenciado pelo desempenho do **Qwen2-Math:1.5B**, que, apesar de menor, apresentou os melhores resultados em sua categoria.
                """)
    
    show_metrics(["qwen2-math:1.5b", "qwen2-math:7b", "mathstral"], example_text_questions)
    
    st.markdown("""
                ### 2.4 - Avalia√ß√£o dos Demais Modelos  

                Por fim, realizamos testes com os modelos restantes focados em **processamento de texto**, al√©m de um modelo com **suporte √† vis√£o**, com o objetivo de avaliar o desempenho de um modelo multimodal. Esses resultados servir√£o como base para compara√ß√µes futuras, especialmente quando analisarmos o desempenho dos modelos em quest√µes que envolvem imagens.  

                Os modelos que obtiveram os melhores resultados em **acur√°cia geral** foram o **Phi-4** e o **Mistral-Small**, ambos apresentando **mais de 70% de precis√£o**. No entanto, observamos que o **Mistral-Small teve um tempo de execu√ß√£o significativamente superior aos demais**, levando **aproximadamente 44 minutos** para concluir o teste. Al√©m disso, os modelos desenvolvidos pela **Microsoft** tamb√©m apresentaram **tempos de infer√™ncia mais elevados**, embora **n√£o t√£o acentuados quanto o Mistral-Small**.  

                Um fen√¥meno incomum foi observado no modelo **Phi-3.5**. Durante as primeiras quest√µes, o modelo demonstrou **bom desempenho**, respondendo de maneira coerente. No entanto, conforme o teste progrediu, o modelo come√ßou a apresentar **alucina√ß√µes**, gerando respostas totalmente distintas das solicitadas. Esse comportamento impactou diretamente o n√∫mero de **respostas nulas**, resultando em uma taxa alarmante de **83% de quest√µes inv√°lidas**.  

                Ao analisarmos o gr√°fico de **correla√ß√£o entre tempo m√©dio e acur√°cia**, constatamos que o modelo que obteve **o melhor equil√≠brio entre desempenho e tempo de execu√ß√£o** foi o **Phi-4**, pois alcan√ßou **excelentes resultados sem comprometer significativamente o tempo de processamento**. Por outro lado, o desempenho do **Mistral-Small** pode ter sido influenciado pela **limita√ß√£o de hardware**. Como o modelo possui um tamanho de **14GB**, mas foi executado em uma **placa de v√≠deo com 12GB de VRAM**, parte do processamento foi deslocado para a **mem√≥ria RAM**, resultando em um maior tempo de carregamento e execu√ß√£o. Esse fator pode ter impactado diretamente sua velocidade e efici√™ncia durante o teste.  

                Ao analisarmos o desempenho por disciplina, os modelos **Phi-4** e **Mistral-Small** surpreenderam ao apresentar **um desempenho superior em matem√°tica** quando comparados aos modelos projetados especificamente para essa √°rea. O **Phi-4**, em particular, atingiu **92% de precis√£o** em matem√°tica, mantendo **um desempenho consistente em todas as demais disciplinas**, o que indica sua **elevada capacidade geral** em rela√ß√£o aos outros modelos testados.  

                Outro dado relevante foi a compara√ß√£o entre o modelo **Mistral** e sua varia√ß√£o especializada em matem√°tica (**Mathstral**). O **Mathstral**, que passou por um processo de fine-tuning voltado para c√°lculos e racioc√≠nio l√≥gico, demonstrou um **desempenho significativamente superior** ao modelo base. Enquanto o **Mathstral** obteve **12 acertos em matem√°tica**, o **Mistral padr√£o acertou apenas 3**, evidenciando uma melhora de **400%**. Esse resultado confirma que o fine-tuning foi **bem-sucedido**, tornando o modelo especializado **mais eficiente na resolu√ß√£o de problemas matem√°ticos**. Em outras disciplinas, no entanto, os dois modelos apresentaram desempenhos variados, com o modelo base tendo um acero a mais em ci√™ncias da natureza e 5 a mais em linguagens, enquando o modelo com fine-tunnig mostrou um desempenho superior, al√©m do j√° mencionado, em ci√™ncias humanas. 

                Um dos modelos que se destacou em termos de **desempenho global** foi o **Phi-4**. Seu desempenho nas diferentes disciplinas foi **consistente e expressivo**. O modelo obteve **92% de acertos em ci√™ncias humanas**, **76% em linguagem**, **84% em ci√™ncias da natureza** e **um impressionante 92% em matem√°tica**, consolidando-se como um dos melhores modelos em termos de **acur√°cia geral**. Esse resultado indica que, mesmo com seu comportamento inst√°vel em longas execu√ß√µes, o **Phi-3.5 demonstrou um n√≠vel de compreens√£o bastante elevado**, sendo capaz de lidar com quest√µes de diferentes √°reas com alto √≠ndice de acertos.  

                De maneira geral, os modelos analisados nesta se√ß√£o demonstraram **desempenhos distintos dependendo do crit√©rio avaliado**. Enquanto **Phi-4** se destacou pelo equil√≠brio entre **tempo de infer√™ncia e acur√°cia**, **Mistral-Small** apresentou resultados robustos, por√©m com um custo computacional elevado. O modelo **Phi-3.5**, por sua vez, revelou um desempenho impressionante em todas as disciplinas, especialmente em matem√°tica, mas sofreu com um n√∫mero elevado de respostas inv√°lidas ao longo do teste. Essas diferen√ßas evidenciam que a escolha do modelo ideal deve levar em considera√ß√£o **n√£o apenas a acur√°cia bruta, mas tamb√©m o tempo de infer√™ncia, estabilidade das respostas e adequa√ß√£o √†s tarefas espec√≠ficas**.                 
                """)
    
    show_metrics(["phi4", "phi3.5", "llava", "llama3.2", "mistral", "mistral-small"], example_text_questions)
    
    st.markdown("""
                ### 2.5 - An√°lise de Desempenho por Disciplina  
                
                Com a finaliza√ß√£o dos testes para cada categoria de modelo, passamos agora a uma an√°lise mais detalhada do **desempenho por disciplina**. O objetivo √© compreender como os diferentes modelos se comportam em cada √°rea do conhecimento, identificando padr√µes, pontos fortes e eventuais limita√ß√µes em suas respostas.  
                """)
    
    all_text_models = ["mistral", "phi4", "phi3.5", "llava", "llama3.2",  "mistral-small", "qwen2-math:1.5b", "qwen2-math:7b", "mathstral", "deepscaler", "deepseek-r1", "mistral-nemo", "openthinker", "smallthinker", "gemma2:2b", "gemma2", "gemma2:27b", "qwen2.5:14b", "qwen2.5:7b", "qwen2.5:1.5b"]
    text_table = utils.tabela_geral(example_text_questions, all_text_models)
    table_disciplinas = utils.analisar_tabela(text_table, 'discipline')
    st.dataframe(table_disciplinas)
    
    
    st.markdown("#### 2.5.1 - Matem√°tica")
    
    show_discipline_metrics(text_table, 'matematica')
    
    st.markdown("""
                Em rela√ß√£o ao tempo de execu√ß√£o, as quest√µes de **matem√°tica** apresentaram um tempo m√©dio significativamente superior ao das demais disciplinas, atingindo em m√©dia 20,48 segundos por quest√£o, enquanto nas outras √°reas do conhecimento nenhuma ultrapassou 8 segundos em m√©dia. Essa diferen√ßa evidencia que os modelos demandam um tempo consideravelmente maior para processar e responder quest√µes matem√°ticas, possivelmente devido √† necessidade de opera√ß√µes aritm√©ticas ou dificuldades na interpreta√ß√£o das express√µes.
                
                Al√©m disso, o tempo m√°ximo registrado para uma √∫nica quest√£o de matem√°tica foi de **542 segundos**, um valor muito superior ao tempo m√°ximo observado em outras disciplinas, cujo segundo maior tempo registrado foi de **173,49 segundos**. Essa discrep√¢ncia sugere que certas quest√µes matem√°ticas exigem **muito mais processamento**, podendo ativar mecanismos internos dos modelos, como **passos adicionais de racioc√≠nio, tentativa de c√°lculos detalhados ou maior esfor√ßo na busca por padr√µes matem√°ticos**. Esse comportamento n√£o foi observado com a mesma intensidade em disciplinas como **linguagens e ci√™ncias humanas**, onde as respostas parecem ser geradas com maior rapidez e consist√™ncia.  
                
                Esses resultados refor√ßam a hip√≥tese de que quest√µes matem√°ticas **imp√µem uma carga computacional maior** aos modelos, tornando sua resolu√ß√£o **mais lenta e sujeita a varia√ß√µes** quando comparada a outras √°reas do conhecimento.  
                """)
    
    st.markdown("#### 2.5.2 - Ci√™ncias Humanas")
    show_discipline_metrics(text_table, 'ciencias-humanas')
    st.markdown("""
                A disciplina de **Ci√™ncias Humanas** apresentou um desempenho **superior √† maioria das demais √°reas**, com uma **acur√°cia m√©dia de 67%** entre os modelos testados. Al√©m disso, o **tempo m√©dio de infer√™ncia** foi relativamente **baixo**, sendo **muito pr√≥ximo ao da disciplina de Linguagens**, com uma diferen√ßa de apenas **0,0139 segundos**.  

                O bom desempenho nessa disciplina pode estar relacionado ao **tipo de quest√£o abordada**, que, em muitos casos, exige uma resposta mais **direta e factual**, reduzindo a necessidade de **interpreta√ß√£o subjetiva**. Isso pode ter facilitado a gera√ß√£o de respostas corretas pelos modelos, diferentemente do que ocorre em disciplinas que exigem um maior n√≠vel de infer√™ncia contextual.  

                Embora a m√©dia geral tenha sido positiva, alguns modelos se destacaram de forma significativa. **Oito modelos** conseguiram acertar **20 ou mais quest√µes**, um resultado expressivo quando comparado a outras disciplinas. Esse dado refor√ßa que **os modelos de linguagem est√£o relativamente bem ajustados para esse tipo de conte√∫do**, conseguindo lidar com conceitos hist√≥ricos, geogr√°ficos e sociais de maneira eficaz.  
                """)
    
    st.markdown("#### 2.5.3 - Linguagens")
    show_discipline_metrics(text_table, 'linguagens')
    
    st.markdown("""
                O desempenho dos modelos na disciplina de **Linguagens** foi inferior ao de Ci√™ncias Humanas, registrando uma **acur√°cia m√©dia de 55,8%**. Apesar disso, foi uma das **melhores m√©tricas entre todas as disciplinas avaliadas**, superando, por exemplo, Matem√°tica e Ci√™ncias da Natureza.  
                
                Acreditamos que essa diferen√ßa de desempenho em rela√ß√£o a Ci√™ncias Humanas pode estar associada √† **necessidade de interpreta√ß√£o mais complexa** das quest√µes. Enquanto muitas perguntas em Ci√™ncias Humanas possuem **respostas factuais mais objetivas**, as quest√µes de Linguagens frequentemente exigem **an√°lise contextual, compreens√£o de nuances textuais e infer√™ncias**, aspectos que podem representar um desafio adicional para os modelos testados.  
                
                Mesmo com uma m√©dia relativamente inferior, **alguns modelos demonstraram desempenho satisfat√≥rio**. Apenas **um modelo** conseguiu acertar **mais de 20 quest√µes**, mas **12 modelos** atingiram **15 ou mais acertos**, o que representa um resultado bastante positivo. Esse desempenho sugere que, embora a disciplina de Linguagens represente um desafio maior em termos de interpreta√ß√£o, **boa parte dos modelos ainda conseguiu alcan√ßar uma performance s√≥lida**, demonstrando certo n√≠vel de capacidade de compreens√£o textual. 
                """)
    
    
    st.markdown("#### 2.5.4 - Ci√™ncias da Natureza")
    show_discipline_metrics(text_table, 'ciencias-natureza')
    st.markdown("""
                A disciplina de Ci√™ncias da Natureza apresentou um desempenho pr√≥ximo ao observado em Linguagens, tanto em rela√ß√£o √† acur√°cia quanto ao tempo de infer√™ncia. No entanto, ao analisarmos a varia√ß√£o de tempo entre os modelos testados, nota-se que a escala de crescimento do tempo de infer√™ncia foi exponencialmente inferior √† observada em Matem√°tica, sugerindo que a complexidade das quest√µes n√£o gerou um impacto t√£o significativo no processamento dos modelos. Em termos m√©dios, os modelos levaram aproximadamente dez segundos a mais para responder √†s quest√µes dessa disciplina quando comparados a Linguagens, enquanto a acur√°cia m√©dia se mostrou apenas um ponto percentual superior.  

                Esse comportamento j√° era, de certa forma, esperado, uma vez que muitas quest√µes de Ci√™ncias da Natureza exigem conhecimento pr√©vio de conceitos espec√≠ficos, assim como ocorre em Ci√™ncias Humanas. Entretanto, diferentemente dessa √∫ltima, a disciplina incorpora um n√∫mero significativo de quest√µes que demandam c√°lculos, aproximando-se, em parte, das exig√™ncias matem√°ticas. Em F√≠sica, por exemplo, modelos precisaram lidar com quest√µes relacionadas √†s leis de Newton e fen√¥menos ondulat√≥rios, enquanto em Qu√≠mica foram exigidos c√°lculos estequiom√©tricos e propor√ß√µes entre reagentes. At√© mesmo em Biologia, verificou-se a necessidade de opera√ß√µes num√©ricas, como c√°lculos de porcentagens em gen√©tica e estat√≠sticas populacionais.  

                Dessa forma, os resultados indicam que Ci√™ncias da Natureza pode ser caracterizada como uma disciplina h√≠brida, combinando elementos de interpreta√ß√£o conceitual e resolu√ß√£o de problemas quantitativos. No entanto, ao considerar o tempo de infer√™ncia e a taxa de acertos, observa-se que o comportamento dos modelos nessa disciplina se assemelha mais ao das demais √°reas do conhecimento do que ao da Matem√°tica, na qual a escalada do tempo de execu√ß√£o se mostrou muito mais acentuada.  
                """)
    
    
    st.markdown("""
                ### 2.6 - Resultados por Quest√µes  

                Nesta se√ß√£o, analisamos o desempenho dos modelos sob a √≥tica das quest√µes individuais, observando a distribui√ß√£o de acertos e o tempo de infer√™ncia m√©dio por quest√£o.  

                Inicialmente, ao considerarmos a distribui√ß√£o de acertos, observamos que, em m√©dia, uma quest√£o foi respondida corretamente por **8,48 modelos**, enquanto a mediana situa-se entre **9 e 15 modelos**. A quest√£o com maior n√∫mero de acertos foi corretamente respondida por mais de **15 modelos diferentes**, evidenciando que algumas perguntas s√£o resolvidas com alto grau de consist√™ncia entre os modelos. Um aspecto interessante √© que **n√£o houve nenhuma quest√£o que n√£o tenha sido corretamente respondida por pelo menos um modelo**, conforme ilustrado na distribui√ß√£o do histograma abaixo. Como pode ser observado, as duas quest√µes com **menor n√∫mero de acertos** foram resolvidas corretamente por apenas **dois modelos**.  

                A an√°lise desse histograma revela um comportamento que **se aproxima de uma distribui√ß√£o normal**, com maior concentra√ß√£o de frequ√™ncia na regi√£o central da curva e menor nas extremidades. No entanto, nota-se uma assimetria pontual, especialmente na **categoria de acertos igual a 4**, que apresentou uma frequ√™ncia superior √† categoria seguinte, o que indica uma leve irregularidade na distribui√ß√£o dos acertos.  
                """)
    
    table_question = utils.analisar_tabela(text_table, 'question')
    table_question = table_question[table_question['Total'] > 10]
    
    st.pyplot(plots.histogram(table_question, 'OK', 100))
    
    st.markdown("Por outro lado, ao analisarmos a distribui√ß√£o do tempo m√©dio de infer√™ncia por quest√£o, observamos um padr√£o **exponencial**, no qual os tempos mais baixos representam a maior parte da distribui√ß√£o. A mediana situa-se em **5,4 segundos**, com a m√©dia em **9,3 segundos** e o terceiro quartil em **10,66 segundos**, enquanto o tempo m√°ximo registrado foi de **45 segundos**. Esse comportamento sugere que, na maioria dos casos, os modelos conseguem responder rapidamente √†s quest√µes, exceto em algumas exce√ß√µes nas quais o tempo de infer√™ncia se eleva consideravelmente. Como analisado anteriormente, essas exce√ß√µes ocorrem com maior frequ√™ncia em **quest√µes matem√°ticas**, que exigem processamento adicional, podendo resultar em tempos significativamente mais elevados.")
    
    st.pyplot(plots.histogram(table_question, 'Tavg', 4))
    
    st.markdown("Para ilustrar melhor esses resultados, a seguir apresentamos exemplos das **quest√µes mais f√°ceis e mais dif√≠ceis**, evidenciando os padr√µes de acerto e tempo de resposta em diferentes tipos de problemas.")
    
    st.markdown("""
                As duas quest√µes que tiveram apenas 1 acerto foram uma de Linguagens e outra de Ci√™ncias da Natureza, respectivamente dos anos de 2013 e 2018.
                
                ##### Qust√£o Dif√≠cil: Quest√£o 2013123 - Linguagens
                
                **Para Carr, internet atua no com√©rcio da distra√ß√£o**

                _Autor de ‚ÄúA Gera√ß√£o Superficial‚Äù analisa a influ√™ncia da tecnologia na mente_

                O jornalista americano Nicholas Carr acredita que a internet n√£o estimula a intelig√™ncia de ningu√©m. O autor explica descobertas cient√≠ficas sobre o funcionamento do c√©rebro humano e teoriza sobre a influ√™ncia da internet em nossa forma de pensar.  
                Para ele, a rede torna o racioc√≠nio de quem navega mais raso, al√©m de fragmentar a aten√ß√£o de seus usu√°rios.  
                Mais: Carr afirma que h√° empresas obtendo lucro com a recente fragilidade de nossa aten√ß√£o. ‚ÄúQuanto mais tempo passamos _on-line_ e quanto mais r√°pido passamos de uma informa√ß√£o para a outra, mais dinheiro as empresas de internet fazem‚Äù, avalia.  
                ‚ÄúEssas empresas est√£o no com√©rcio da distra√ß√£o e s√£o _experts_ em nos manter cada vez mais famintos por informa√ß√£o fragmentada em partes pequenas. √â claro que elas t√™m interesse em nos estimular e tirar vantagem da nossa compuls√£o por tecnologia.‚Äù

                ROXO, E. **Folha de S. Paulo**, 18 fev. 2012 (adaptado).

                (A) : Mant√©m os usu√°rios cada vez menos preocupados com a qualidade da informa√ß√£o.

                (B) : Torna o racioc√≠nio de quem navega mais raso, al√©m de fragmentar a aten√ß√£o de seus usu√°rios.

                (C) : Desestimula a intelig√™ncia, de acordo com descobertas cient√≠ficas sobre o c√©rebro.

                (D) : Influencia nossa forma de pensar com a superficialidade dos meios eletr√¥nicos.

                (E) : Garante a empresas a obten√ß√£o de mais lucro com a recente fragilidade de nossa aten√ß√£o.

                | **Resposta:** (E)
                
                Por fim, tivermos 5 quest√µes com mais respostas corretas, das quais, n√£o houve nenhuma de matem√°tica, por√©m, em Linguagens tivemos a quest√£o de id **2011109**, em Humanas tivemos as quest√µes **2021090, 2010033, 2023050**, e em Ci√™ncias da Natureza tivemos a quest√£o **2021097**.
                
                ##### 2.6.2.1 Quest√£o mais f√°cil Linguagens

                O tema da velhice foi objeto de estudo de brilhantes fil√≥sofos ao longo dos tempos. Um dos melhores livros sobre o assunto foi escrito pelo pensador e orador romano C√≠cero: _A Arte do Envelhecimento_. C√≠cero nota, primeiramente, que todas as idades t√™m seus encantos e suas dificuldades. E depois aponta para um paradoxo da humanidade. Todos sonhamos ter uma vida longa, o que significa viver muitos anos. Quando realizamos a meta, em vez de celebrar o feito, nos atiramos a um estado de melancolia e amargura. Ler as palavras de C√≠cero sobre envelhecimento pode ajudar a aceitar melhor a passagem do tempo.

                NOGUEIRA, P. Sa√∫de & Bem-Estar Antienvelhecimento. **√âpoca**. 28 abr. 2008.

                (A) : Esclarecer que a velhice √© inevit√°vel.

                (B) : Contar fatos sobre a arte de envelhecer.

                (C) : Defender a ideia de que a velhice √© desagrad√°vel.

                (D) : Influenciar o leitor para que lute contra o envelhecimento.

                (E) : Mostrar √†s pessoas que √© poss√≠vel aceitar, sem ang√∫stia, o envelhecimento.

                | Resposta: E

                ##### 2.6.2.1 Quest√£o mais f√°cil Humanas

                EIGENHEER, E. M. **Lixo:** a limpeza urbana atrav√©s dos tempos. Porto Alegre: Gr√°fica Palloti, 2009.

                **Texto II**  
                A repugnante tarefa de carregar lixo e os dejetos da casa para as pra√ßas e praias era geralmente destinada ao √∫nico escravo da fam√≠lia ou ao de menor status ou valor. Todas as noites, depois das dez horas, os escravos conhecidos popularmente como ‚Äútigres‚Äù levavam tubos ou barris de excremento e lixo sobre a cabe√ßa pelas ruas do Rio.

                KARACH, M. C. **A vida dos escravos no Rio de Janeiro, 1808-1850.** Rio de Janeiro: Cia. das letras, 2000.
                (A) : Valoriza√ß√£o do trabalho bra√ßal.

                (B) : Reitera√ß√£o das hierarquias sociais.

                (C) : Sacraliza√ß√£o das atividades laborais.

                (D) : Supera√ß√£o das exclus√µes econ√¥micas.

                (E) : Ressignifica√ß√£o das heran√ßas religiosas.

                | Resposta: B

                ##### 2.6.2.2 Quest√£o mais f√°cil Natureza

                EIGENHEER, E. M. **Lixo:** a limpeza urbana atrav√©s dos tempos. Porto Alegre: Gr√°fica Palloti, 2009.

                **Texto II**  
                A repugnante tarefa de carregar lixo e os dejetos da casa para as pra√ßas e praias era geralmente destinada ao √∫nico escravo da fam√≠lia ou ao de menor status ou valor. Todas as noites, depois das dez horas, os escravos conhecidos popularmente como ‚Äútigres‚Äù levavam tubos ou barris de excremento e lixo sobre a cabe√ßa pelas ruas do Rio.

                KARACH, M. C. **A vida dos escravos no Rio de Janeiro, 1808-1850.** Rio de Janeiro: Cia. das letras, 2000.
                (A) : Valoriza√ß√£o do trabalho bra√ßal.

                (B) : Reitera√ß√£o das hierarquias sociais.

                (C) : Sacraliza√ß√£o das atividades laborais.

                (D) : Supera√ß√£o das exclus√µes econ√¥micas.

                (E) : Ressignifica√ß√£o das heran√ßas religiosas.

                | Resposta: B
                """)
    
    st.divider()
    
    st.markdown("""
                ### **2.7 - Conclus√£o**  
                """)
    
    show_metrics(all_text_models,example_text_questions)
    
    st.markdown("""
                A partir dos experimentos conduzidos, foi poss√≠vel obter uma vis√£o detalhada sobre o desempenho de diferentes modelos de linguagem na resolu√ß√£o de quest√µes de m√∫ltipla escolha, abrangendo diversos aspectos como **acur√°cia geral, tempo de infer√™ncia, desempenho por disciplina e comportamento frente a diferentes tipos de tarefas**. Os resultados indicam que **o n√∫mero de par√¢metros nem sempre √© o principal fator determinante da performance**, visto que **modelos intermedi√°rios apresentaram desempenhos compar√°veis a modelos robustos**, enquanto modelos especializados nem sempre demonstraram superioridade nas √°reas para as quais foram ajustados.  

                Na an√°lise do impacto do **tamanho do modelo**, observamos que, enquanto modelos menores apresentaram quedas significativas na acur√°cia, a diferen√ßa entre modelos intermedi√°rios e robustos foi **m√≠nima ou inexistente**. Em alguns casos, como no **Gemma2**, a vers√£o intermedi√°ria obteve tempos de infer√™ncia menores sem perda expressiva de precis√£o, sugerindo que **h√° um ponto de equil√≠brio entre efici√™ncia computacional e desempenho**.  

                O **tempo de infer√™ncia** demonstrou ser um fator cr√≠tico, especialmente em modelos **de reasoning e matem√°tica**, que frequentemente apresentaram tempos muito superiores aos modelos convencionais. O **Openthinker**, por exemplo, chegou a ultrapassar **1h24min** de execu√ß√£o para 100 quest√µes, enquanto os demais modelos raramente ultrapassaram **15 minutos**. Em Matem√°tica, a **demanda computacional** foi um grande desafio, com algumas quest√µes exigindo **at√© 542 segundos** para serem resolvidas, um valor muito superior ao de qualquer outra disciplina.  

                A an√°lise **por disciplina** revelou padr√µes interessantes. **Ci√™ncias Humanas e Linguagens** foram as √°reas de maior acur√°cia, com **67% e 55,8% de precis√£o m√©dia**, respectivamente. O desempenho superior em Ci√™ncias Humanas pode estar relacionado ao fato de que muitas quest√µes exigem respostas mais **diretas e factuais**, reduzindo a necessidade de **interpreta√ß√£o subjetiva**. J√° em Linguagens, a exig√™ncia de **an√°lise contextual** e infer√™ncias mais complexas pode ter dificultado a obten√ß√£o de acertos. **Ci√™ncias da Natureza**, por sua vez, apresentou um comportamento h√≠brido, combinando aspectos de interpreta√ß√£o conceitual com quest√µes que exigiam c√°lculos.  

                A **Matem√°tica**, como esperado, foi a disciplina de maior dificuldade, tanto em acur√°cia quanto em tempo de infer√™ncia. Apenas **quatro modelos** conseguiram acertar mais da metade das quest√µes matem√°ticas, e o tempo m√©dio de resposta foi **mais que o dobro** das demais disciplinas. Mesmo os modelos especializados em matem√°tica n√£o demonstraram ganhos significativos de desempenho na √°rea, com exce√ß√£o do **Mathstral**, que superou o Mistral padr√£o em um fator de **4x**, evidenciando o impacto positivo do fine-tuning.  

                Quando analisamos **o desempenho por quest√£o**, verificamos que **nenhuma quest√£o foi completamente ignorada por todos os modelos**, o que indica um n√≠vel m√≠nimo de compreens√£o em todas as √°reas avaliadas. No entanto, houve uma clara distin√ß√£o entre quest√µes **f√°ceis e dif√≠ceis**, com algumas sendo acertadas por **mais de 15 modelos** e outras por **apenas dois modelos**. O histograma das respostas revelou uma **distribui√ß√£o pr√≥xima da normalidade**, embora com algumas irregularidades, o que pode indicar que certos tipos de perguntas s√£o consistentemente mais desafiadores para os modelos.  

                Os resultados estat√≠sticos refor√ßaram essas observa√ß√µes. O **teste t de Student** n√£o encontrou **diferen√ßa significativa** entre modelos intermedi√°rios e robustos, sugerindo que modelos menores podem ser op√ß√µes vi√°veis em determinados contextos. No entanto, o **teste de equival√™ncia (TOST)** n√£o foi capaz de confirmar que os modelos est√£o dentro de um intervalo de toler√¢ncia de **¬±2%**, o que impede uma conclus√£o definitiva sobre a equival√™ncia entre categorias de modelos.  

                Diante desses resultados, podemos concluir que **a escolha do modelo ideal deve levar em considera√ß√£o m√∫ltiplos fatores al√©m da acur√°cia bruta**, incluindo **tempo de infer√™ncia, consumo computacional, estabilidade das respostas e adequa√ß√£o √†s tarefas espec√≠ficas**. Modelos robustos podem ser necess√°rios para cen√°rios que demandam **maior precis√£o**, mas modelos intermedi√°rios demonstraram **efici√™ncia suficiente** para grande parte das tarefas. Al√©m disso, a dificuldade que os modelos enfrentaram com **quest√µes matem√°ticas** refor√ßa a necessidade de avan√ßos na capacidade de racioc√≠nio l√≥gico das arquiteturas atuais.  

                Por fim, a an√°lise apresentada destaca **os desafios e limita√ß√µes das arquiteturas de LLMs atuais**, ao mesmo tempo que aponta caminhos para otimiza√ß√µes futuras, seja na **especializa√ß√£o de modelos para tarefas espec√≠ficas**, seja no aprimoramento do **balan√ßo entre custo computacional e desempenho**.  
                """)